---
layout: post
title:  "2018-10-10-支持向量机原理及利用python实现"
category: 'bigdata'
tags: 机器学习 支持向量机 python
author: y570pc
image: http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex8materials/ex8a_fill.png
---

* content
{:toc}

## 朴素贝叶斯原理

#### 原理 

定义超平面$h(x)=\omega^Tx+b$，样本$x$到超平面的几何间隔$r=\frac{\omega^Tx+b}{\|\|\omega\|\|}$，其中
$||\omega||=\sqrt{\omega_0^2+\omega_1^2+...+\omega_n^2}$。

 
在支持向量机中把几何间隔$r$作为最大化间隔进行分析，并采用-1和1作为类别标签。我们把其中一个支撑向量$x^*$到最优超平面的距离定义为：

$$
r^*=\frac{h(x^*)}{||\omega||}=
\begin{cases}
\frac{1}{||\omega||}& if: y^*=h(x^*)=+1\\
-\frac{1}{||\omega||}& if: y^*=h(x^*)=-1
\end{cases}
$$

对于任意空间点$x_i$，有约束条件：

$$
\begin{cases}
\omega^Tx_i+b\geq1&   y_i=+1\\
\omega^Tx_i+b\leq-1&   y_i=-1
\end{cases}
$$

优化问题变为：

$$
\begin{cases}
\mathop{max}\limits_{\omega,b}\frac{2}{||\omega||} \iff \mathop{min}\limits_{\omega,b}\frac{1}{2}|\omega||^2\\
y_i(\omega^Tx_i+b)\geq1,(i=1,2,...,n)
\end{cases}
$$


#### 拉格朗日乘子发对偶问题

上式的优化目标是二次的，约束是线性的，整体是一个凸二次规划问题。将其转化为拉格朗日对偶问题后更容易求解，也方便后面引入核函数。

$$
L(x)=f(x)+\sum\alpha g(x)
$$

其中，$f(x)$是目标函数，$g(x)$是约束条件。为了得到拉格朗日函数的最优解。

有n个点设置对应的拉格朗日乘子$\alpha_i\ge 0$，得到原始问题的拉格朗日函数：

$$
L(\omega,b,\alpha)=\frac{1}{2}||\omega||^2+\sum_{i=1}^n\alpha_i[1-y_i(\omega^Tx_i+b)]
$$

目标是让$L(\omega,b,\alpha)$针对$\alpha$达到最大值，然后再让$L(\omega,b,\alpha)$针对$\omega,b$达到最小值。目标函数

$$
\mathop{min}\limits_{\omega,b}\mathop{max}\limits_{\alpha}L(\omega,b,\alpha)
$$

其对偶问题是：

$$
\mathop{max}\limits_{\alpha}\mathop{min}\limits_{\omega,b}L(\omega,b,\alpha)
$$

需满足下列KKT条件才能有解：

$$
\begin{cases}
\alpha_i\geq0\\
y_i(\omega^Tx_i+b)-1\geq0\\
\alpha_i[y_i(\omega^Tx+b)-1]=0
\end{cases}
$$

#### $\omega,b$的部分
$L(\omega,b,\alpha)$对$\omega$和$b$分别求偏导，并令其等于0，带入原拉格朗日式子得到对偶问题：

$$
\begin{cases}
\mathop{max}\limits_{\alpha}\sum_{i=1}^{n}\alpha_i-\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
s.t.\qquad\sum_{i=1}^{n}\alpha_iy_i=0\\
\alpha_i\geq0\\
\end{cases}
$$ 

#### $\alpha$的部分

利用SMO算法求解，就是将最大优化问题分解为多个小优化问题来求解，这些小优化问题往往很容易求解，并且对他们进行顺序求解的结果与将他们作为一个整体求解的结果完全一致。

#### 软间隔

引入松弛变量，允许模型出现某些错误。

$$
\begin{cases}
\mathop{min}\limits_{\omega,b}\frac{1}{2}|\omega||^2+C\sum_{i=1}^n\xi_i\\
y_i(\omega^Tx_i+b)\geq1-\xi_i,\xi_i\geq0,(i=1,2,...,n)
\end{cases}
$$

得到新的拉格朗日函数

$$
L(\omega,b,\xi,\alpha,\mu)=\frac{1}{2}||\omega||^2+C\sum_{i=1}^n\xi_i+\sum_{i=1}^n\alpha_i[1-\xi_i-y_i(\omega^Tx_i+b)]-\sum_{i=1}^n\mu_i\xi_i
$$

需满足下列KKT条件才能有解：

$$
\begin{cases}
\alpha_i\geq0\\,\mu_i\geq0
y_i(\omega^Tx_1+b)-1+\xi_i\geq0\\
\alpha_i[y_i(\omega^Tx_i+b)-1+\xi_i]=0\\
\xi_i\geq0,\mu_i\xi_i=0
\end{cases}
$$

$L(\omega,b,\xi,\alpha,\mu)$对$\omega,b和\xi$分别求偏导，并令其等于0，带入原拉格朗日式子得到对偶问题：

$$
\begin{cases}
\mathop{max}\limits_{\alpha}\sum_{i=1}^{n}\alpha_i-\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
s.t.\qquad\sum_{i=1}^{n}\alpha_iy_i=0\\
0\leq\alpha_i\leq C
\end{cases}
$$ 

## 核函数

在现实任务中，可能并不存在一个超平面将数据集完美地线性分割开，这样需要将原始空间映射到一个更高维的空间，如果在这一高伟空间数据集变得线性可分，那么问题就得到了解决。

超平面$\omega^T{\color{red} {\phi(x)}}+b=0$

经过推导得到：

$$
\begin{cases}
\mathop{max}\limits_{\alpha}\sum_{i=1}^{n}\alpha_i-\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j{\color{red} {\phi(x_i)^T\phi(x_j)}}\\
s.t.\qquad \sum_{i=1}^{n}\alpha_iy_i=0\\
\alpha_i\geq0\\
\end{cases}
$$ 

如果存在一个函数$\kappa(x_i,x_j)=\phi(x_i)^T\phi(x_j)$就可以在低维空间计算高维空间的点积结果。称$\kappa(x_i,x_j)$为**核函数**。

常见的核函数有：

| 名称 | 表达式 | 备注 |
| :------:| :------: | :------: |
| 线性核 | $\kappa(x_i,x_j)=x_i^Tx_j$ |  |
| 多项式核 | $\kappa(x_i,x_j)=(x_i^Tx_j)^d$ | $d\geq1$为多项式次数 |
| 高斯核 | $\kappa(x_i,x_j)=exp(-\frac{\vert\vert x_i-x_j\vert\vert^2}{2\sigma^2}$ | $\sigma>0$为高斯核宽带 |
| 拉普拉斯核 | $\kappa(x_i,x_j)=exp(-\frac{\vert\vert x_i-x_j\vert\vert}{\sigma}$ | $\sigma>0$ |
| Sigmoid核| $\kappa(x_i,x_j)=tanh(\beta x_i^Tx_j+\theta)$ | $\beta>0,\theta<0,tanh$为双曲正切函数 |

## 参考资料
[1]. [SVM原理以及Tensorflow 实现SVM分类(附代码)](https://www.cnblogs.com/vipyoumay/p/7560061.html)

[2]. [【机器学习】支持向量机SVM原理及推导](https://blog.csdn.net/u014433413/article/details/78427574)