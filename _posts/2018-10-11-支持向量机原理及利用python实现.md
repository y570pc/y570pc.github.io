---
layout: post
title:  "2018-10-10-朴素贝叶斯原理及利用python实现垃圾邮件识别"
categories: 大数据
tags: 机器学习 朴素贝叶斯 python
author: y570pc
---

* content
{:toc}

## 朴素贝叶斯原理

#### 原理 

定义超平面$h(x)=\omega^Tx+b$，样本$x$到超平面的几何间隔$r=\frac{\omega^Tx+b}{\|\|\omega\|\|}$，其中
$||\omega||=\sqrt{\omega_0^2+\omega_1^2+...+\omega_n^2}$。

 
在支持向量机中把几何间隔$r$作为最大化间隔进行分析，并采用-1和1作为类别标签。我们把其中一个支撑向量$x^*$到最优超平面的距离定义为：

$$
r^*=\frac{h(x^*)}{||\omega||}=
\begin{cases}
\frac{1}{||\omega||}& if: y^*=h(x^*)=+1\\
-\frac{1}{||\omega||}& if: y^*=h(x^*)=-1
\end{cases}
$$

对于任意空间点$x_i$，有约束条件：

$$
\begin{cases}
\omega^Tx_i+b\geq1&   y_i=+1\\
\omega^Tx_i+b\leq-1&   y_i=-1
\end{cases}
$$

优化问题变为：

$$
\begin{cases}
\mathop{max}\limits_{\omega,b}\frac{2}{||\omega||} \iff \mathop{min}\limits_{\omega,b}\frac{1}{2}|\omega||^2\\
y_i(\omega^Tx_i+b)\geq1,(i=1,2,...,n)
\end{cases}
$$


#### 拉格朗日橙子发对偶问题

上式的优化目标是二次的，约束是线性的，整体是一个凸二次规划问题。将其转化为拉格朗日对偶问题后更容易求解，也方便后面引入核函数。

$$
L(x)=f(x)+\sum\alpha g(x)
$$

其中，$f(x)$是目标函数，$g(x)$是约束条件。为了得到拉格朗日函数的最优解。

有m个点设置对应的拉格朗日乘子$\alpha_i\ge0，得到原始问题的拉格朗日函数：

$$
L(\omega,b,\alpha)=\frac{1}{2}||\omega||^2+\sum_{i=1}^n\alpha_i[1-y_i(\omega^Tx_i+b)]
$$

目标是让$L(\omega,b,\alpha)$针对$\alpha$达到最大值，然后再让$L(\omega,b,\alpha)$针对$\omega,b$达到最小值。目标函数

$$
\mathop{min}\limits_{\omega,b}\mathop{max}\limits_{\alpha}L(\omega,b,\alpha)
$$

其对偶问题是：

$$
\mathop{max}\limits_{\alpha}\mathop{min}\limits_{\omega,b}L(\omega,b,\alpha)
$$

需满足下列条件才能有解：

$$
\begin{cases}
\alpha_i\geq0\\
y_i(\omega^Tx+b)-1\geq0\\
\alpha_i[y_i(\omega^Tx+b)-1]=0
\end{cases}
$$

#### $\omega,b$的部分
$L(\omega,b,\alpha)$对$\omega$和$b$分别求偏导，并令其等于0，带入原拉格朗日式子得：

$$
\begin{cases}
W(\alpha)=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
\sum_{i=1}^{n}\alpha_iy_i=0\\
\alpha_i\geq0\\
\end{cases}
$$ 

#### $\alpha$的部分

利用SMO算法求解，就是将最大优化问题分解为多个小优化问题来求解，这些小优化问题往往很容易求解，并且对他们进行顺序求解的结果与将他们作为一个整体求解的结果完全一致。

#### 软间隔

引入松弛变量，允许模型出现某些错误。

$$
\begin{cases}
\mathop{min}\limits_{\omega,b}\frac{1}{2}|\omega||^2+C\sum_{i=1}^n\xi_i\\
y_i(\omega^Tx_i+b)\geq1-\xi_i,\xi_i\geq0,(i=1,2,...,n)
\end{cases}
$$

对偶问题

$$
\begin{cases}
W(\alpha)=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
\sum_{i=1}^{n}\alpha_iy_i=0\\
0\leq\alpha_i\leq C
\end{cases}
$$ 


## 参考资料
[1]. [SVM原理以及Tensorflow 实现SVM分类(附代码)](https://www.cnblogs.com/vipyoumay/p/7560061.html)

[2]. [【机器学习】支持向量机SVM原理及推导](https://blog.csdn.net/u014433413/article/details/78427574)