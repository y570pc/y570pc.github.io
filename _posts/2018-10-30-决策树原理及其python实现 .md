---
layout: post
title:  "决策树原理及其python实现"
category: 'bigdata'
tags: 机器学习 决策树 CART 
author: y570pc
image: "/img/2018-10-30-01.jpg"
---

## 原理

#### 简介

决策树学习：根据数据的属性采用树状结构建立决策模型。决策树模型常常用来解决分类和回归问题。常见的算法包括 CART (Classification And Regression Tree)、ID3、C4.5、随机森林 (Random Forest) 等。比如ID3算法使用信息增益作为不纯度；C4.5算法使用信息增益率作为不纯度【此方法避免了ID3算法中的归纳偏置问题，因为ID3算法会偏向于选择类别较多的属性（形成分支较多会导致信息增益大）】；CART算法使用基尼系数作为不纯度。

<table align="center">
	<tr  bgcolor="#F6F8FA" >
		<th>算法</th>
		<th>支持模型</th>
		<th>数结构</th>
		<th>特征选择</th>
		<th>连续值</th>
		<th>缺失值</th>
		<th>剪枝</th>
	</tr>
	<tr>
		<td>ID3</td>
		<td>分类</td>
		<td>多叉树</td>
		<td>信息增益</td>
		<td>不支持</td>
		<td>不支持</td>
		<td>不支持</td>
	</tr>
	<tr>
		<td>C4.5</td>
		<td>分类</td>
		<td>多叉树</td>
		<td>信息增益率</td>
		<td>支持</td>
		<td>支持</td>
		<td>支持</td>
	</tr>
	<tr>
		<td>CART</td>
		<td>分类、回归</td>
		<td>二叉树</td>
		<td>基尼系数</td>
		<td>支持</td>
		<td>支持</td>
		<td>支持</td>
	</tr>
</table>

#### 算法

**ID3算法**

利用**熵**来衡量信息集的无序程度。

假设某节点S包含s个样本，共有m个类别，分别对应$C_i,i\in{1,2,...,m}$，一个类别中包含样本数为$s_i$，此时确定节点中任意一个样本的类别所需的信息量（信息熵）为： 

$$
I(s_1,s_2,...,s_m)=-\sum_{i=1}^{m}p_i*log(p_i)
$$

其中$p_i$表示任一样本属于类别$C_i$的概率，即$p_i=\frac{s_i}{s}$。

假设特征A有v个不同取值，${a_1,a_2,...,a_v}$，那么利用特征A可以将该节点样本划分为v个子集。$S_j$包含了集合S中特征A取$a_j$值的样本集合，对应的样本数为$s_j$。假设$S_{ij}$为子集$S_j$属于类别$C_i$的样本集合，对应的样本数目为$s_{ij}$，那么在子集$S_j$中确定任一样本类别所需的信息熵为：

$$
I(s_{1j},s_{2j},...,s_{mj})=-\sum_{i=1}^{m}p_{ij}*log(p_{ij})
$$

单独在各子集进行样本类别确定所需信息熵的的加权平均：

$$
E(A)=\sum_{j=i}^v\frac{s_{1j}+s_{2j}+...+s_{mj}}{s}*I(s_{1j},s_{2j},...,s_{mj})
$$

这样利用特征A对当前节点的样本进行划分子集的信息增益为：

$$
Gain(A)=I(s_1,s_2,...,s_m)-E(A)
$$

**C4.5算法**

ID3算法倾向于选择水平数量较多的变量，可能导致训练得到一个庞大且深度浅的树，C4.5算法对此进行了改进。

$$
GainRatio(A)=\frac{Gain(A)}{SplitInfo(A)}

SplitInfo(A)=-\sum_{i=1}^{v}\frac{s_i}{s}log_2(\frac{s}{s_i})

SplitInfo(A)反映的是按照特征A对样本进行划分的广度和均匀度。
$$

**CART分类树算法**

使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。

假设有K个类别，第k个类别的概率为$p_k$，则基尼系数的表达式为：

$$
Gini(p)=\sum_{k=i}^Kp_k*(1-p_k)=1-=\sum_{k=i}^Kp_k^2
$$

**CART回归树算法**

实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。一种可行的方法是将数据集切分成很多份易建模的数据，然后利用线性回归技术来建模。

对于回归模型，我们使用了常见的和方差的度量特征的各个划分点的优劣，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为： 

$$
min_{A,s}[min_{c_1}\sum_{x_i\in D_1(A,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in D_2(A,s)}(y_i-c_2)^2]

其中，c_1为D1数据集的样本输入均值，c_2为D2数据集的样本输出均值。
$$

#### 优缺点

优点：
* 分类规则清晰，结果容易理解；
* 计算量相对较小，实现速度快；
* 同时可以处理分类变量和数值变量（但是可能决策树对连续变量的划分并不合理，所以可以提前先离散化）；
* 另外决策树不需要做变量筛选，它会自动筛选，适合处理高维度数据；
* 可以处理异常值、缺失值。

缺点：
* 不是很稳点，数据变化一点，树就会发生变化；
* 没有考虑变量之间相关性，每次筛选都只考虑一个变量（因此不需要归一化）；
* 只能线性分割数据；
* 容易出现过拟合；
* 贪婪算法（可能找不到最好的树）；
* 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。

#### 树枝修剪

让树的完全长成，但这会出现过拟合问题。为了抑制这种情况的发生，需要给树的剪枝。

树的剪枝分为预剪枝和后剪枝，预剪枝，及早的停止树增长控制树的规模。

后剪枝在已生成过拟合决策树上进行剪枝，删除没有意义的组，可以得到简化版的剪枝决策树，包括REP（Reduced-Error Pruning），设定一定的误分类率，减掉对误分类率上升不超过阈值的多余枝；还有一种CCP，即给分裂准则加上惩罚项，此时树的层数越深，基尼系数的惩罚项会越大。

## 参考文献

[1]. [算法模型---决策树](https://blog.csdn.net/qingqing7/article/details/78416708)

